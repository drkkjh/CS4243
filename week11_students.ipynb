{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a297ba0",
   "metadata": {},
   "source": [
    "# CS4243 - Lab 11\n",
    "Computer Vision & Pattern Recognition\n",
    "\n",
    "Week 11\n",
    "\n",
    "Author: Dr. Amirhassan MONAJEMI. Modified by: Soo Han"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de6ad1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function estimation using neural network, libraries\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from numpy import asarray\n",
    "from matplotlib import pyplot as plt\n",
    "import math as m\n",
    "import random as r\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935a36fe",
   "metadata": {},
   "source": [
    "# CS4243, Deep Learning Image Classification Example\n",
    "\n",
    "In this lab, you will go through a general example of deep learning models for image classification which is similar to your mini-project. This example uses a dataset of cat and dog images to solve a binary classification task.\n",
    "\n",
    "This example is provided by Prof Amir, with additional notes and pointers written on this notebook by Soo Han."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbce734",
   "metadata": {},
   "source": [
    "## Part 1: Setting up\n",
    "\n",
    "We first check to see if we can use GPUs for faster training. If you have GPU but do not have a version that uses GPU, please use the following code block below:\n",
    "\n",
    "```\n",
    "python3 -m pip install tensorflow[and-cuda]\n",
    "# Verify the installation:\n",
    "python3 -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95807556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please install GPU version of TF\n"
     ]
    }
   ],
   "source": [
    "# We will do a simple check to see if we have GPU for training. Please use GPU to accelerate your training.\n",
    "\n",
    "if tf.test.gpu_device_name(): \n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113bdab1",
   "metadata": {},
   "source": [
    "### Downloading our dataset\n",
    "Download the zip file `ann_images1.zip` from our google drive. Alternatively, you can download the file from Canvas/Files/Python notebooks/set6. Unzip the file to obtain 2 directories containing images for train and test. `pets_very_small` folder will be used to train our model, and `pets_tiny_test` will be used for testing of the model. The list of test images can be found in `flst.txt` file. Please \n",
    "\n",
    "If you are using Google Colab, modify and run the additional codeblock provided for you to mount the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bcf197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the train and evaluation dataset directories. Change the below to your own path\n",
    "train_path = '/home/soohan/4243/sample_images/pets_very_small'\n",
    "test_path = '/home/soohan/4243/sample_images/pets_tiny_test'\n",
    "imglist_path = '/home/soohan/4243/sample_images/flst.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b530c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "To run this code on co lab: \n",
    "\n",
    "add: import os\n",
    "\n",
    "add: \n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "!ls\n",
    "\n",
    "set the directory, e.g.:\n",
    "\"/content/gdrive/MyDrive/ANN/pets_very_small\"\n",
    "\"/content/gdrive/MyDrive/ANN/flst.txt\"\n",
    "\n",
    "flst.txt file should be modified too\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dabac4",
   "metadata": {},
   "source": [
    "## Part 2: Train our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52926cd1",
   "metadata": {},
   "source": [
    "We first load data using the `tf.keras.utils.image_dataset_from_directory` utility. We split the images from the train directory into two, to be used for training of the model and validating its performance (note that it is not for testing the final performance of the model).\n",
    "\n",
    "You can view the output from these datasets which we loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b07ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (256,256)\n",
    "batch_size = 16\n",
    "\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    train_path,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=110,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    train_path,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=110,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771afc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# showing the images \n",
    "#\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1):\n",
    "    for i in range(4):\n",
    "        ax = plt.subplot(2, 2, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(int(labels[i]))\n",
    "        plt.axis(\"off\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb5ce68",
   "metadata": {},
   "source": [
    "We add in some data augmentation which provides several benefits for training Convolutional Neural Networks (CNNs) such as model invariance (flipping an image horizontally can help the model recognize objects irrespective of their orientation (left or right). Similarly, rotations can help a model recognize objects that might be tilted in various ways), better generalizations (slight rotations, zooming, or changes in lighting conditions can make the model more robust to such natural variations when making predictions on new, unseen data) and more. You can search up on the details of these augmentations from the keras documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a909084f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augmentation, using horizontal flip, and random rotation \n",
    "# rotation factor is between 0 to 0.1*2pi \n",
    "# \n",
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
    "        layers.experimental.preprocessing.RandomRotation(0.1),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030bb8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# showing the rotated and flipped images that were added to the original dataset\n",
    "#\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, _ in train_ds.take(1):\n",
    "    for i in range(4):\n",
    "        augmented_images = data_augmentation(images)\n",
    "        ax = plt.subplot(2,2, i + 1)\n",
    "        plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08326c7a",
   "metadata": {},
   "source": [
    "We define our model here which is essentially a Convolutional Neural Network. If you are not familiar with CNNs, I would recommend reading this <a href=\"https://aigents.co/data-science-blog/publication/introduction-to-convolutional-neural-networks-cnns\">article</a> and this fun <a href=\"https://setosa.io/ev/image-kernels/\">playground</a> (full credits to their corresponding authors.\n",
    "\n",
    "Some key points to help you understand some components below:\n",
    "- Input() is used to instantiate a Keras tensor. It is more of a symbolic use rather than it meaning a mathematical operation - it's a way to define how the input data to the model should look.\n",
    "- Batch normalization normalizes the activations of a layer to have zero mean and unit variance, helping to stabilize and accelerate training by reducing internal covariate shift.\n",
    "- Residual refers to residual connections which allow gradients to \"skip\" layers by adding the original input to the output, helping to mitigate the vanishing gradient problem and enabling deeper networks to be trained more effectively. \n",
    "\n",
    "FYI: Prof Amir uses Separable Convolutions layers here instead of the normal Conv2D layers you might be used to. Just note that this is just a variant of Convolution Layers, and at your own time may read up more about it. One resource is this <a href=\"https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-b99ec3102728\">link</a>. Understanding this specific layer is not the objective of today's lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d5ebf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(input_shape):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    # Image augmentation block\n",
    "    x = data_augmentation(inputs)\n",
    "\n",
    "    # Entry block\n",
    "    x = layers.experimental.preprocessing.Rescaling(1.0 / 255)(x)\n",
    "    x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    x = layers.Conv2D(64, 3, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    previous_block_activation = x  # Set aside residual\n",
    "    for size in [128, 256, 512, 728]:\n",
    "        \n",
    "        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        \n",
    "        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        \n",
    "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "        # Project residual\n",
    "        residual = layers.Conv2D(size, 1, strides=2, padding=\"same\")(\n",
    "            previous_block_activation\n",
    "        )\n",
    "        x = layers.add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "    x = layers.SeparableConv2D(1024, 3, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    activation = \"sigmoid\"\n",
    "    units = 1\n",
    "   \n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Dense(25, activation='relu')(x)\n",
    "    outputs = layers.Dense(units, activation=activation)(x)\n",
    "    return keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224f89e1",
   "metadata": {},
   "source": [
    "As we have learnt from week 10 lab session, we define our model, `compile` to configure our ANN's learning process, and use the `fit` method to start the training process of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b3fc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_model(input_shape=image_size + (3,) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03898ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling and training our model\n",
    "\n",
    "epochs = 80\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-3),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af43a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a handy function in Keras that lets you look at your model which you have just compiled. \n",
    "# Personally, looking at the output shape is particularly useful when you do CV\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197f06cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_ds, epochs=epochs, validation_data=val_ds,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f2317c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy curve\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28ab7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss curve\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e61983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the classifier with some images  \n",
    "flst = np.loadtxt(imglist_path, dtype=np.character) \n",
    "ddmm = len(flst)\n",
    "tags = np.zeros( (1,ddmm) )\n",
    "tags[:,27:ddmm]= 1\n",
    "tags = np.int8( tags.T )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0d5f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# the list of test files is in flst.txt, you may need to change the path \n",
    "# predict() function is employed , each test image is preprocessed the way train images had been\n",
    "# \n",
    "predct = []\n",
    "for i in flst:\n",
    "    i = i.decode('utf-8')\n",
    "    ### If your image directories are note in the same place as your notebook, you can use this c:\n",
    "    # i = i.replace('../pets_tiny_test', test_path)\n",
    "    ###\n",
    "    img = keras.preprocessing.image.load_img( i , target_size=image_size )\n",
    "    img_array = keras.preprocessing.image.img_to_array(img)\n",
    "    img_array = tf.expand_dims(img_array, 0)  # Create batch axis\n",
    "\n",
    "    predictions = model.predict(img_array, verbose=0)\n",
    "    score = predictions[0]\n",
    "    print( i, \n",
    "        \" is %.2f percent cat and %.2f percent dog.\"\n",
    "        % (100 * (1 - score), 100 * score)\n",
    "    )\n",
    "    predct.append( np.round(score) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac42795",
   "metadata": {},
   "outputs": [],
   "source": [
    "predct = np.int8( np.array(predct) )\n",
    "sscc = np.sum(abs(tags-predct))\n",
    "print('Number of correct classification =' , ddmm-sscc , ' out of ', ddmm , ' means ', round((ddmm-sscc)/ddmm,3) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53317ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you wanna save your model: later you can load it using 'load_model' instruction\n",
    "#model.save('path_to_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821ae80f",
   "metadata": {},
   "source": [
    "## Part 3: Try these suggested changes to your model and compare the validation performance!\n",
    "\n",
    " 1. set the base model parameters to image size = 256x256, epochs=100 , dropout=0.5\n",
    " 2. train and test the model. see the performance, training, validation, and testing accuracy\n",
    " 3. go to \"make_model\" function cell\n",
    " 4. change all the 'relu' activation function to 'sigmoid'\n",
    " 5. run the program, train and test it and see the performances. better? same? worse?\n",
    " 6. bring the activation functions back to the 'relu'\n",
    " 7. at the end of \"make_model\" function, find this lines and make that line a comment: \n",
    "    ```\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Dense(25, activation='relu')(x)\n",
    "    outputs = layers.Dense(units, activation=activation)(x)\n",
    "    return keras.Model(inputs, outputs)\n",
    "    ```\n",
    "        change to:\n",
    "    ```\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    # x = layers.Dense(25, activation='relu')(x)\n",
    "    outputs = layers.Dense(units, activation=activation)(x)\n",
    "    return keras.Model(inputs, outputs)\n",
    "    ```\n",
    "     This means that we are going to have just 1 output neuron and no fully-connected classification layer \n",
    " 8. run the model and see the performance. Compared to the base model, is it better? same? worse?\n",
    " 9. bring back all the modifications \n",
    " 10. at \"make_model\" function, find this lines and make that line a comment: \n",
    "    ```\n",
    "    previous_block_activation = x  # Set aside residual\n",
    "    for size in [128, 256, 512, 728]:\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "    ```\n",
    "       change to:\n",
    "    ```\n",
    "    previous_block_activation = x  # Set aside residual\n",
    "    for size in [128, 256, 512, 728]:\n",
    "        #x = layers.Activation(\"relu\")(x)\n",
    "        #x = layers.SeparableConv2D(size, 3, padding=\"same\")(x)\n",
    "        #x = layers.BatchNormalization()(x)\n",
    "    ```\n",
    "    \n",
    " it means that we are going to remove one of the convolution modules\n",
    " 11. run the model and see the performance. Compared to the base model, is it better? same? worse?\n",
    "\n",
    "\n",
    "You may run the training multiple times to be sure of your results!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
