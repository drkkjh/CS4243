{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9383e5a3",
   "metadata": {},
   "source": [
    "## CS4243, Week 12 Lab Plan\n",
    "### 2023/24- S1\n",
    "### Part 1: Ask Your Mini project-related questions, \n",
    "\n",
    "\n",
    "## \n",
    "### Part 2: Ask Your CW3-related questions, \n",
    "\n",
    "\n",
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9e8b51",
   "metadata": {},
   "source": [
    "## Part 3: Object Detection Using Yolo v3\n",
    "### Amir - 2023\n",
    "#### A few good resources: https://pjreddie.com/  ,  https://www.datacamp.com/blog/yolo-object-detection-explained  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8882e083",
   "metadata": {},
   "source": [
    "### Download the YOLO pre-trained weights and configuration file:\n",
    "\n",
    "- YOLO has several versions (YOLOv1, YOLOv2, YOLOv3, etc.).\n",
    "- We focus on Yolo v3 \n",
    "- you can find YOLOv3 weights and configuration files (yolov3.cfg and yolov3.weights) in Canvas/ Files/ Set6 folder and place them in your project directory.\n",
    "- You can use OpenCV to load the YOLO model using the cv2.dnn.readNet function. Make sure you provide the paths to the configuration and weights files you downloaded earlier. \n",
    "- dnn stands for deep neural network. openCV employs that as a class for deep learning model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9032a3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLO\n",
    "net = cv2.dnn.readNet(\"./yolo/yolov3.cfg\", \"./yolo/yolov3.weights\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e246688",
   "metadata": {},
   "source": [
    "Now net cotains a trained Yolo v3 model\n",
    "YOLO is trained on the COCO (Common Objects in Context) dataset, s\n",
    "o you'll need to load the class labels to map the class IDs to human-readable labels. \n",
    "You can download the labels again from Canvas/ Files/ Set6 folder\n",
    "Although they are all available on the web too.\n",
    "COCO contains: \n",
    "\n",
    "####  person , bicycle, car, motorbike, aeroplane, bus, train, truck, boat, bench, cat, dog, horse, sheep, cow, ....\n",
    "would be saved in variable classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6725fa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./yolo/coco.names\", \"r\") as f:\n",
    "    classes = f.read().strip().split(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce98fa6",
   "metadata": {},
   "source": [
    "#### Now you can try any image to see how Yolo works. \n",
    "#### go for images with a few known, medium-size objects inside \n",
    "### images to try (test image):\n",
    "1. still_life_light_photo.jpg \n",
    "2. 16335.jpg \n",
    "3. offset_1094232.jpg\n",
    "4. IMG_20211204_123437.jpg\n",
    "5. 0841b64699e5f9d1c1bb628b11a50151.jpg \n",
    "\n",
    "all avialble at Canvas, same directory. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e7be46",
   "metadata": {},
   "source": [
    "#### 3 important variable that are going to be generated here:\n",
    "1. blob: generated by dnn.blobFromImage , contains the blobs detected in the test image. blob will be the input of our net\n",
    "2. layer_names which is the unconnected outlayers of the net\n",
    "3. detections: answers generated by your net when the input is the test image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfbc358",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4d0632ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2592 3888\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "image = cv2.imread(\"./yolo/IMG_7471.JPG\")\n",
    "# image = cv2.imread(\"../../../Downloads/0200869_220904_threat_6503_240.png\")\n",
    "height, width, _ = image.shape\n",
    "print(height, width)\n",
    "blob = cv2.dnn.blobFromImage(image, 1/255.0, (416, 416), swapRB=True, crop=False)\n",
    "net.setInput(blob)\n",
    "\n",
    "layer_names = net.getUnconnectedOutLayersNames()\n",
    "detections = net.forward(layer_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a14d62",
   "metadata": {},
   "source": [
    "#### Let's see what we have got in detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87d7df3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'> 3 507\n",
      "(array([[0.03899167, 0.05419095, 0.37402165, ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.04278925, 0.03642969, 0.29820463, ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.04677168, 0.04191811, 0.8526776 , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       ...,\n",
      "       [0.96287316, 0.9529423 , 0.38496146, ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.96774906, 0.96623874, 0.33863387, ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.964474  , 0.9641712 , 0.83607894, ..., 0.        , 0.        ,\n",
      "        0.        ]], dtype=float32), array([[0.01910068, 0.02556489, 0.05088575, ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.01888695, 0.01664926, 0.41781428, ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.02112211, 0.01654611, 0.07129858, ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       ...,\n",
      "       [0.97384554, 0.9773058 , 0.05041542, ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.982808  , 0.9763013 , 0.37776506, ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.981514  , 0.98559356, 0.07389233, ..., 0.        , 0.        ,\n",
      "        0.        ]], dtype=float32), array([[0.00981032, 0.00622506, 0.01692496, ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.00868748, 0.01031552, 0.01904668, ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.00938328, 0.00775114, 0.22364554, ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       ...,\n",
      "       [0.98904294, 0.9910723 , 0.01693472, ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.99076754, 0.98802483, 0.01847702, ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.98770946, 0.9911857 , 0.17221893, ..., 0.        , 0.        ,\n",
      "        0.        ]], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "print( type(detections) , len(detections) , len(detections[0]))\n",
    "print(detections)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ac6c02",
   "metadata": {},
   "source": [
    "#### Process the detections:\n",
    "You can iterate through the detections and filter out objects with confidence \n",
    "scores above a certain threshold. Then, you can draw bounding boxes and labels on the image to \n",
    "visualize the detected objects.\n",
    "\n",
    "- detections is  3d array, contains detected objects features\n",
    "- detection is a row of detections, means a detected object\n",
    "- object is the features of the detected object\n",
    "\n",
    "#### detections: (N, N\\*grid_height\\*grid_width, 85), where:\n",
    "\n",
    "- N: The number of anchor boxes per grid cell. YOLOv3 uses three anchor boxes by default, so N is 3.\n",
    "- 85: Each detection is represented by 85 values, which include:\n",
    "- - 4 values for the bounding box coordinates (center_x, center_y, width, height).\n",
    "- - 1 value for the objectness score (how likely it is an object).\n",
    "- - 80 values for class probabilities (there are 80 classes in the COCO dataset). maximum shows to most probabel class of the object\n",
    "\n",
    "- grid_height and grid_width: These represent the grid dimensions used for dividing the input image. YOLO divides the image into a grid and performs object detection within each grid cell. Usually it is 13x13 or so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c1a6b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for detection in detections:\n",
    "    for obj in detection:\n",
    "        scores = obj[5:]\n",
    "        class_id = np.argmax(scores)\n",
    "        confidence = scores[class_id]\n",
    "\n",
    "        if confidence > 0.5:  # You can adjust the confidence threshold\n",
    "            center_x = int(obj[0] * width)\n",
    "            center_y = int(obj[1] * height)\n",
    "            w = int(obj[2] * width)\n",
    "            h = int(obj[3] * height)\n",
    "\n",
    "            # Calculate the coordinates of the bounding box\n",
    "            x = int(center_x - w/2)\n",
    "            y = int(center_y - h/2)\n",
    "\n",
    "            # Draw bounding box and label\n",
    "            cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            label = f\"{classes[class_id]}: {confidence:.2f}\"\n",
    "            cv2.putText(image, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "263778b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.namedWindow(\"YOLO O D\", cv2.WINDOW_NORMAL)\n",
    "cv2.imshow(\"YOLO O D\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec894d0b",
   "metadata": {},
   "source": [
    "### What you may do ...\n",
    "1. Run it using different test images\n",
    "2. Realize its advantages and disadvantages\n",
    "3. Try different confidence coeeficients/thresholds\n",
    "4. Realize what is the argmax of the few first detected objects\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f83e845",
   "metadata": {},
   "source": [
    "#### Bye ...."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
